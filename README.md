## TRPO理解

在TRPO算法中，$r_t(\theta)$ 表示代理在状态 $s_t$ 选择动作 $a_t$ 的概率比率，即使用当前策略 $\pi_\theta(a_t|s_t)$ 选择动作 $a_t$ 的概率与使用旧策略 $\pi_{\theta_{old}}(a_t|s_t)$ 选择动作 $a_t$ 的概率之比，即：
$$
r_t(\theta)=\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)}
$$
如果 $r_t(\theta)>1$，则说明当前策略比旧策略更好；如果 $r_t(\theta)<1$，则说明当前策略更差。

TRPO算法的优化目标是最大化一个“替代”目标函数（surrogate objective function），该函数的表达式为：
$$
L^{s u r}(\theta)=\mathbb{E}_t\left[r_t(\theta) A_t\right]
$$
其中 $\mathbb{E}_t$ 表示对时间步 $t$ 的期望，$A_t$ 表示优势函数的估计值，用于衡量在状态 $s_t$ 采取动作 $a_t$ 相对于采取平均动作的价值提升程度。优势函数的估计值通常使用 GAE (Generalized Advantage Estimation) 方法计算得到。

TRPO算法最大化替代目标函数的同时，需要满足一个约束条件，即当前策略 $\pi_\theta(a_t|s_t)$ 与旧策略 $\pi_{\theta_{old}}(a_t|s_t)$ 的 KL 散度不超过一个指定的阈值。通过限制策略更新的幅度，TRPO算法可以确保策略改进的稳定性和效率。





## 优势函数的表达式

在 PPO 算法中，优势函数（Advantage Function）用于衡量在当前状态下采取某个动作相对于平均情况的价值提升，即一个动作相对于其他动作的优劣程度。

优势函数的表达式通常是：

$A_t = Q_t - V_t$

其中 $Q_t$ 表示在时间步 $t$ 采取某个动作后能够获得的总回报，$V_t$ 表示在时间步 $t$ 采取随机策略时期望能够获得的总回报。$A_t$ 表示在时间步 $t$ 采取某个动作相对于随机策略的价值提升。

在实际应用中，$Q_t$ 和 $V_t$ 都无法直接获取，需要通过近似方法进行估计。在 PPO 算法中，$Q_t$ 和 $V_t$ 可以通过价值网络来估计，$V_t$ 可以直接通过价值网络输出的值来获取，而 $Q_t$ 则需要进一步通过优势函数来计算。具体来说，$Q_t$ 的表达式可以写成：

$Q_t = r_t + \gamma V_{t+1}$

其中 $r_t$ 表示在时间步 $t$ 采取某个动作后获得的即时奖励，$\gamma$ 表示折扣因子，$V_{t+1}$ 表示在时间步 $t+1$ 时采取随机策略的期望回报。将 $Q_t$ 的表达式代入优势函数的定义式中，可以得到：

$A_t = Q_t - V_t = r_t + \gamma V_{t+1} - V_t$

这个表达式就是在 PPO 算法中通常所采用的优势函数的表达式。在具体实现中，可以使用 PyTorch 提供的张量运算函数来计算优势函数的值，例如 `torch.sub()` 函数可以用于计算两个张量的差值，`torch.mul()` 函数可以用于计算两个张量的乘积。



