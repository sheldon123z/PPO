{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_getting_started.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldon123z/PPO/blob/main/1_getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Stable Baselines3 Tutorial - Getting Started\n",
        "\n",
        "Github repo: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
        "\n",
        "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn the basics for using stable baselines library: how to create a RL model, train it and evaluate it. Because all algorithms share the same interface, we will see how simple it is to switch from one algorithm to another.\n",
        "\n",
        "\n",
        "## Install Dependencies and Stable Baselines3 Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3).\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c25769bc-1498-4b40-a22c-cf8870b65811"
      },
      "source": [
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install pyglet==1.4"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 freeglut3-dev xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 977 kB of archives.\n",
            "After this operation, 3,350 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 freeglut3-dev amd64 2.8.1-3 [124 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 xvfb amd64 2:1.20.13-1ubuntu1~20.04.6 [780 kB]\n",
            "Fetched 977 kB in 1s (883 kB/s)\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "(Reading database ... 128276 files and directories currently installed.)\n",
            "Preparing to unpack .../freeglut3_2.8.1-3_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-3) ...\n",
            "Selecting previously unselected package freeglut3-dev:amd64.\n",
            "Preparing to unpack .../freeglut3-dev_2.8.1-3_amd64.deb ...\n",
            "Unpacking freeglut3-dev:amd64 (2.8.1-3) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.20.13-1ubuntu1~20.04.6_amd64.deb ...\n",
            "Unpacking xvfb (2:1.20.13-1ubuntu1~20.04.6) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-3) ...\n",
            "Setting up xvfb (2:1.20.13-1ubuntu1~20.04.6) ...\n",
            "Setting up freeglut3-dev:amd64 (2.8.1-3) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-1.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.8/171.8 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from stable-baselines3[extra]) (1.4.4)\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from stable-baselines3[extra]) (1.22.4)\n",
            "Collecting importlib-metadata~=4.13\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.9/dist-packages (from stable-baselines3[extra]) (1.13.1+cu116)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.9/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Collecting ale-py==0.7.4\n",
            "  Downloading ale_py-0.7.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich\n",
            "  Downloading rich-13.3.2-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.7/238.7 KB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from stable-baselines3[extra]) (8.4.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.9/dist-packages (from stable-baselines3[extra]) (2.11.2)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from stable-baselines3[extra]) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from stable-baselines3[extra]) (5.9.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (from stable-baselines3[extra]) (4.7.0.72)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.9/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.27.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (8.1.3)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.6.0.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 KB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata~=4.13->stable-baselines3[extra]) (3.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.2.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.16.2)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.19.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.51.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (63.4.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.11->stable-baselines3[extra]) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->stable-baselines3[extra]) (4.39.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->stable-baselines3[extra]) (23.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->stable-baselines3[extra]) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->stable-baselines3[extra]) (2022.7.1)\n",
            "Collecting pygments<3.0.0,>=2.13.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown-it-py<3.0.0,>=2.2.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.2)\n",
            "Collecting libtorrent\n",
            "  Downloading libtorrent-2.0.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for gym\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for gym\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.0-py3-none-any.whl size=446686 sha256=33156c160ac1d565a6a2618ba1f666feadf3b4c61a76b1364726d384196dbb3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/17/c9/c31922a6aaf4ec7ec90eeee5dbc40ffbaafeda64b30a208b72\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Failed to build gym\n",
            "Installing collected packages: libtorrent, pygments, mdurl, importlib-metadata, gym, markdown-it-py, AutoROM.accept-rom-license, autorom, ale-py, stable-baselines3, rich\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Running setup.py install for gym ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: gym was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed AutoROM.accept-rom-license-0.6.0 ale-py-0.7.4 autorom-0.4.2 gym-0.21.0 importlib-metadata-4.13.0 libtorrent-2.0.7 markdown-it-py-2.2.0 mdurl-0.1.2 pygments-2.14.0 rich-13.3.2 stable-baselines3-1.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyglet==1.4\n",
            "  Downloading pyglet-1.4.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from pyglet==1.4) (0.18.3)\n",
            "Installing collected packages: pyglet\n",
            "Successfully installed pyglet-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcX8hEcaUpR0"
      },
      "source": [
        "Stable-Baselines3 works on environments that follow the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html).\n",
        "You can find a list of available environment [here](https://gym.openai.com/envs/#classic_control).\n",
        "\n",
        "It is also recommended to check the [source code](https://github.com/openai/gym) to learn more about the observation and action space of each env, as gym does not have a proper documentation.\n",
        "Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "source": [
        "import gym\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae32CtgzTG3R"
      },
      "source": [
        "The first thing you need to import is the RL model, check the documentation to know what you can use on which problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7tKaBFrTR0a"
      },
      "source": [
        "from stable_baselines3 import PPO"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0_8OQbOTTNT"
      },
      "source": [
        "The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n",
        "This step is optional as you can directly use strings in the constructor: \n",
        "\n",
        "```PPO('MlpPolicy', env)``` instead of ```PPO(MlpPolicy, env)```\n",
        "\n",
        "Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommened option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROUJr675TT01"
      },
      "source": [
        "from stable_baselines3.ppo.policies import MlpPolicy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## Create the Gym env and instantiate the agent\n",
        "\n",
        "For this example, we will use CartPole environment, a classic control problem.\n",
        "\n",
        "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
        "\n",
        "Cartpole environment: [https://www.gymlibrary.dev/environments/classic_control/cart_pole/](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n",
        "\n",
        "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n",
        "\n",
        "\n",
        "We chose the MlpPolicy because the observation of the CartPole task is a feature vector, not images.\n",
        "\n",
        "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
        "\n",
        "Here we are using the [Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) algorithm, which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance).\n",
        "\n",
        "It combines ideas from [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).\n",
        "\n",
        "PPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\n",
        "It is usually less sample efficient than off-policy alorithms like [DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html) or [TD3](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html), but is much faster regarding wall-clock time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUWGZp3i9wyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8959c913-9c53-4d11-cecf-27d561732136"
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "model = PPO(MlpPolicy, env, verbose=0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  class ObservationWrapper(Wrapper):\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efFdrQ7MBvl"
      },
      "source": [
        "We create a helper function to evaluate the agent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63M8mSKR-6Zt"
      },
      "source": [
        "def evaluate(model, num_episodes=100):\n",
        "    \"\"\"\n",
        "    Evaluate a RL agent\n",
        "    :param model: (BaseRLModel object) the RL Agent\n",
        "    :param num_episodes: (int) number of episodes to evaluate it\n",
        "    :return: (float) Mean reward for the last num_episodes\n",
        "    \"\"\"\n",
        "    # This function will only work for a single Environment\n",
        "    env = model.get_env()\n",
        "    all_episode_rewards = []\n",
        "    for i in range(num_episodes):\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        while not done:\n",
        "            # _states are only useful when using LSTM policies\n",
        "            action, _states = model.predict(obs)\n",
        "            # here, action, rewards and dones are arrays\n",
        "            # because we are using vectorized env\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "\n",
        "    mean_episode_reward = np.mean(all_episode_rewards)\n",
        "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
        "\n",
        "    return mean_episode_reward"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK"
      },
      "source": [
        "Let's evaluate the un-trained agent, this should be a random agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDHLMA6NFk95",
        "outputId": "e7ecc81c-cb9f-4907-cdae-bb2c9fe725e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Random Agent, before training\n",
        "mean_reward_before_train = evaluate(model, num_episodes=1000)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: 23.004 Num episodes: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjjPxrwkYJ2i"
      },
      "source": [
        "Stable-Baselines already provides you with that helper:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z6K9YImYJEx"
      },
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oPTHjxyZSOL"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## Train the agent and evaluate it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4cfSXIB-pTF"
      },
      "source": [
        "# Train the agent for 10000 steps\n",
        "model.learn(total_timesteps=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygl_gVmV_QP7"
      },
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00W6yY3NkHG"
      },
      "source": [
        "Apparently the training went well, the mean reward increased a lot ! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Prepare video recording"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "We will record a video using the [VecVideoRecorder](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you will learn about those wrapper in the next notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOObbeu5MMlR"
      },
      "source": [
        "### Visualize trained agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iATu7AiyMQW2",
        "outputId": "68acb027-6c94-4389-8456-2cfb11494814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "record_video('CartPole-v1', model, video_length=500, prefix='ppo-cartpole')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving video to  /content/videos/ppo-cartpole-step-0-to-step-500.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n4i-fW3NojZ"
      },
      "source": [
        "show_videos('videos', prefix='ppo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y8zg4V566qD"
      },
      "source": [
        "## Bonus: Train a RL Model in One Line\n",
        "\n",
        "The policy class to use will be inferred and the environment will be automatically created. This works because both are [registered](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaOPfOrwWEP4"
      },
      "source": [
        "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrI6f5fWnzp-"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook we have seen:\n",
        "- how to define and train a RL model using stable baselines3, it takes only one line of code ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ji3gbNDkf7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}